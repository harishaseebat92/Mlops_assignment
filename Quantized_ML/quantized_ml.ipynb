{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.quantization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets,transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation to normalize the Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL images to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with mean and std dev for MNIST\n",
    "])\n",
    "mnist_train = datasets.MNIST(root='./data', \n",
    "                             train=True, \n",
    "                             transform=transform, \n",
    "                             download=True)\n",
    "mnist_test = datasets.MNIST(root='./data', \n",
    "                            train=False, \n",
    "                            transform=transform, \n",
    "                            download=True)\n",
    "\n",
    "X_train = mnist_train.data\n",
    "y_train = mnist_train.targets\n",
    "X_test = mnist_test.data\n",
    "y_test = mnist_test.targets\n",
    "\n",
    "X_train = X_train.view(X_train.size(0), -1).float() \n",
    "X_test = X_test.view(X_test.size(0), -1).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build custom module for logistic regression\n",
    "class LogisticRegressionModel(torch.nn.Module):    \n",
    "    # build the constructor\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(n_inputs, n_outputs)\n",
    "    # make predictions\n",
    "    # def forward(self, x):\n",
    "    #     y_pred = torch.sigmoid(self.linear(x))\n",
    "    #     return y_pred\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]  # no. of features (64 for digits dataset)\n",
    "num_classes = 10  # (10 for digits dataset)\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "model = LogisticRegressionModel(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.6872\n",
      "Epoch [20/100], Loss: 1.6825\n",
      "Epoch [30/100], Loss: 1.6778\n",
      "Epoch [40/100], Loss: 1.6740\n",
      "Epoch [50/100], Loss: 1.6705\n",
      "Epoch [60/100], Loss: 1.6671\n",
      "Epoch [70/100], Loss: 1.6635\n",
      "Epoch [80/100], Loss: 1.6601\n",
      "Epoch [90/100], Loss: 1.6559\n",
      "Epoch [100/100], Loss: 1.6503\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_model(model, X_train, y_train, criterion, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "train_model(model, X_train, y_train, criterion, optimizer, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.24%, Model Size: 0.03 MB, Inference Time: 0.003699 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "    model_size = sum(p.numel() * (4 if p.dtype == torch.float32 else 1) for p in model.parameters())\n",
    "\n",
    "    return accuracy, model_size, inference_time\n",
    "\n",
    "accuracy, model_size, inference_time = evaluate_model(model, X_test, y_test)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%, Model Size: {model_size / 1e6:.2f} MB, Inference Time: {inference_time:.6f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "def quantize_model(model):\n",
    "    torch.backends.quantized.engine = 'qnnpack'  # Set the quantization backend\n",
    "    quantized_model = quantize_dynamic(\n",
    "        model,  # The model to be quantized\n",
    "        {torch.nn.Linear},  # Specify layers to be quantized (only Linear here)\n",
    "        dtype=torch.qint8  # The target dtype for the quantized model\n",
    "    )\n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_quantized_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    X_test_tensor = X_test.view(X_test.size(0), -1).float()  # Flatten the test data\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    model_size_bytes = sum(p.numel() * (4 if p.dtype == torch.float32 else 1) for p in model.parameters())\n",
    "    model_size_mb = model_size_bytes / (1024 * 1024)  # Convert model size to MB\n",
    "    \n",
    "    return accuracy, model_size_mb, inference_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "quantized engine QNNPACK is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m quantize_model(model)  \n\u001b[0;32m      3\u001b[0m accuracy_quant, model_size_quant, inference_time_quant \u001b[38;5;241m=\u001b[39m evaluate_quantized_model(quantized_model, X_test, y_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantized Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_quant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Model Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size_quant\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB, Quantized Inference Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_time_quant\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m, in \u001b[0;36mquantize_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantize_model\u001b[39m(model):\n\u001b[1;32m----> 4\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqnnpack\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Set the quantization backend\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     quantized_model \u001b[38;5;241m=\u001b[39m quantize_dynamic(\n\u001b[0;32m      6\u001b[0m         model,  \u001b[38;5;66;03m# The model to be quantized\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear},  \u001b[38;5;66;03m# Specify layers to be quantized (only Linear here)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mqint8  \u001b[38;5;66;03m# The target dtype for the quantized model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quantized_model\n",
      "File \u001b[1;32mc:\\Users\\karak\\anaconda3\\Lib\\site-packages\\torch\\backends\\quantized\\__init__.py:38\u001b[0m, in \u001b[0;36m_QEngineProp.__set__\u001b[1;34m(self, obj, val)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__set__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, val: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_qengine(_get_qengine_id(val))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: quantized engine QNNPACK is not supported"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(model)  \n",
    "\n",
    "accuracy_quant, model_size_quant, inference_time_quant = evaluate_quantized_model(quantized_model, X_test, y_test)\n",
    "print(f'Quantized Accuracy: {accuracy_quant}, Model Size: {model_size_quant:.2f} MB, Quantized Inference Time: {inference_time_quant:.6f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
